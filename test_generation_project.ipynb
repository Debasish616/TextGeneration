{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fc4eeef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import dependencies\n",
    "import numpy\n",
    "import sys\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from tensorflow.python import keras \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7bb14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "# loading data and opening our input data in the form of a text file\n",
    "#Project Gutenburg/berg\n",
    "file = open(\"frankenstein.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0d989f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization\n",
    "# standardization\n",
    "def tokenize_words(input):\n",
    "    input = input.lower()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(input)\n",
    "    filtered = filter(lambda token: token not in stopwords.words('english'), tokens)\n",
    "    return \" \".join(filtered)\n",
    "\n",
    "processed_inputs = tokenize_words(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "700b79b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chars to number\n",
    "chars = sorted(list(set(processed_inputs)))\n",
    "char_to_num = dict((c,i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5bb700ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 269566\n",
      "Total vocab: 38\n"
     ]
    }
   ],
   "source": [
    "#check if words to chars or chars to num (?!) has worked?\n",
    "input_len = len(processed_inputs)\n",
    "vocab_len = len(chars)\n",
    "print(\"Total number of characters:\", input_len)\n",
    "print(\"Total vocab:\", vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8db540a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seq length\n",
    "seq_length = 100\n",
    "x_data = []\n",
    "y_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a297873f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns: 269466\n"
     ]
    }
   ],
   "source": [
    "# loop through the sequence\n",
    "for i in range (0, input_len - seq_length,1):\n",
    "    in_seq = processed_inputs[i:i + seq_length]\n",
    "    out_seq = processed_inputs[i + seq_length]\n",
    "    x_data.append([char_to_num[char] for char in in_seq])\n",
    "    y_data.append(char_to_num[out_seq])\n",
    "    \n",
    "n_patterns = len(x_data)\n",
    "print(\"Total Patterns:\", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a6b90199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert input sequence to np arrary and so on\n",
    "X = numpy.reshape(x_data,(n_patterns, seq_length, 1))\n",
    "X = X/float(vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "44f4a3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding\n",
    "y = utils.to_categorical(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "73450582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the model\n",
    "model =  Sequential()\n",
    "model.add(LSTM(256, input_shape = (X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1],activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3e9c9268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the  model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "82ac0f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving weights\n",
    "filepath = \"model_weights_saved.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose = 1, save_best_only = True, mode='min')\n",
    "desired_callbacks = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8b182a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1053/1053 [==============================] - 98s 90ms/step - loss: 2.9430\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.85177, saving model to model_weights_saved.hdf5\n",
      "Epoch 2/4\n",
      "1053/1053 [==============================] - 95s 90ms/step - loss: 2.6706\n",
      "\n",
      "Epoch 00002: loss improved from 2.85177 to 2.64745, saving model to model_weights_saved.hdf5\n",
      "Epoch 3/4\n",
      "1053/1053 [==============================] - 95s 90ms/step - loss: 2.5833\n",
      "\n",
      "Epoch 00003: loss improved from 2.64745 to 2.56404, saving model to model_weights_saved.hdf5\n",
      "Epoch 4/4\n",
      "1053/1053 [==============================] - 95s 91ms/step - loss: 2.4971\n",
      "\n",
      "Epoch 00004: loss improved from 2.56404 to 2.47797, saving model to model_weights_saved.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f76a98d7130>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit model and let it train\n",
    "model.fit(X,y, epochs=4, batch_size=256, callbacks=desired_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1a13994f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recompile model with the saved weights\n",
    "filename = \"model_weights_saved.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "fae3bf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output of the model back into the characters\n",
    "num_to_char = dict((i,c) for i,c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d05c4354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed: \n",
      "\"  man whose name beaufort proud unbending disposition could bear live poverty oblivion country former \"\n"
     ]
    }
   ],
   "source": [
    "# random seed to help generate\n",
    "start = numpy.random.randint(0, len(x_data) - 1)\n",
    "pattern = x_data[start]\n",
    "print(\"Random Seed: \")\n",
    "print(\"\\\"\", ''.join([num_to_char[value] for value in pattern]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b92f7a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " seare serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer serer ser"
     ]
    }
   ],
   "source": [
    "# generate the text\n",
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern,(1,len(pattern), 1))\n",
    "    x = x/float(vocab_len)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = num_to_char[index]\n",
    "    seq_in = [num_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern =  pattern[1:len(pattern)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887cf46a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
